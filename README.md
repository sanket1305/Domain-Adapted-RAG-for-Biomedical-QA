# Retrieval-Augmented Biomedical Language Modeling  
### Domain-Adaptive Fine-Tuning + RAG for Biomedical Summarization and QA

## ğŸ“Œ Overview
Biomedical documents are long, terminology-dense, and fact-critical. While large language models (LLMs) generate fluent text, they often suffer from **hallucinations**, **poor domain grounding**, and **long-context limitations** when applied directly to biomedical tasks.

This project investigates how **Domain-Adaptive Fine-Tuning (DAPT)** combined with **Retrieval-Augmented Generation (RAG)** can improve factual correctness, semantic relevance, and reliability in biomedical summarization and question answering.

We progressively move from a baseline summarization model to a scalable, retrieval-grounded system evaluated on controlled biomedical benchmarks.

---

## ğŸ¯ Objectives
- Evaluate baseline biomedical summarization performance
- Adapt language models to biomedical domains efficiently
- Reduce hallucinations using external knowledge retrieval
- Study retrieval depth and token-efficiency trade-offs
- Build a reproducible, deployable RAG-based biomedical NLP pipeline

---

## ğŸ§ª Experimental Roadmap

### Phase 1: Baseline Biomedical Summarization (Motivation)
**Model:** T5-small  
**Dataset:** PubMed articleâ€“abstract pairs  

This phase establishes a clean baseline and exposes core challenges:
- Limited capacity for biomedical terminology
- High computational cost with weak semantic alignment
- Low factual consistency in generated summaries  

**Outcome:** Identified the need for domain adaptation and retrieval-based grounding.

---

### Phase 2: Scalable Fine-Tuning + Retrieval-Augmented Summarization
**Model:** Phi-3 Mini (3.8B)  
**Techniques:**
- LoRA fine-tuning
- Q4_K_M quantization (llama.cpp)
- Local inference via Ollama
- RAG using Sentence Transformers + FAISS (MedQA corpus)

We evaluated model performance **before and after retrieval augmentation**, observing:
- Improved ROUGE and BERTScore metrics
- Reduced hallucination
- Better factual alignment through retrieved evidence

**Key Insight:** Retrieval contributes more to reliability than fine-tuning alone.

---

### Phase 3: Controlled Biomedical QA with Domain Adaptation + RAG
**Model:** T5-base  
**Dataset:** PubMedQA  

This phase validates the approach on a structured QA benchmark.

**Key Components:**
- **Domain-Adaptive Pre-Training (DAPT):** Span corruption on unlabeled PubMedQA data
- **Pseudo-Label Distillation:** Teacher LLM generates silver labels + explanations
- **RAG-augmented training and inference**

**Results:**
- Accuracy improvement with RAG
- Significant gains on the hardest class ("maybe")
- Higher Macro-F1 score

**Retrieval Study:**  
Focused retrieval (top-k = 1) consistently outperformed higher k values, which introduced noise under token constraints.

---

## ğŸ“Š Evaluation Metrics
- ROUGE-1 / ROUGE-2 / ROUGE-L
- BERTScore (semantic similarity)
- Accuracy
- Macro-F1
- Class-wise performance analysis

---

## ğŸ§  Key Takeaways
- Domain adaptation improves fluency but **retrieval is the primary driver of factual grounding**
- Efficient fine-tuning (LoRA + quantization) enables large-model performance under limited compute
- Focused retrieval outperforms broad retrieval in token-limited settings
- RAG pipelines are essential for trustworthy biomedical NLP systems

---

## ğŸ› ï¸ Tech Stack
- **Models:** T5-small, T5-base, Phi-3 Mini
- **Fine-Tuning:** LoRA, DAPT
- **Retrieval:** Sentence Transformers, FAISS
- **Deployment:** Ollama, llama.cpp
- **Evaluation:** ROUGE, BERTScore, Accuracy, F1
- **Frameworks:** PyTorch, Hugging Face Transformers

---

## ğŸ‘¤ Author Contributions
**Sanket Deshmukh**
- Designed and implemented Domain-Adaptive Pre-Training (DAPT)
- Built pseudo-label distillation pipeline using teacher LLMs
- Developed end-to-end RAG infrastructure
- Conducted retrieval-depth and performance trade-off analysis
- Led evaluation and experimental analysis

---

## ğŸ“¦ Data Files

The data files used for this project can be found [here](https://drive.google.com/drive/folders/1L1EPMndxMfQgBtV2_pV3FcqP6NLJF92R?usp=drive_link)

---

## ğŸ“ Project Structure
```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ embeddings/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ fine_tuned/
â”‚   â””â”€â”€ quantized/
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ faiss_index/
â”‚   â””â”€â”€ embedding_pipeline.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ dapt.py
â”‚   â”œâ”€â”€ finetune_lora.py
â”‚   â””â”€â”€ pseudo_labeling.py
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ rouge_eval.py
â”‚   â”œâ”€â”€ bertscore_eval.py
â”‚   â””â”€â”€ qa_metrics.py
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ rag_inference.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“Œ Future Work
- Larger-scale DAPT on full PubMed Central
- Multi-document retrieval fusion strategies
- Faithfulness-specific metrics (FactCC, QAGS)
- Clinical decision-support extensions

---

## ğŸ“„ License
This project is released under the **MIT License** for academic and research use.

---

## â­ Acknowledgements
- PubMed / PubMedQA datasets
- Hugging Face Transformers
- FAISS and Sentence Transformers
- Microsoft Phi model family